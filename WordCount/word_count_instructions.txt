Lab Steps
1	Define a New Package
1.1	Right-click on the WordCount project and select New -> Source Folder.
1.2	Enter src /main/java for the name and click the Finish button.
1.3	Right-click on the src /main/java folder and select New -> Package.
1.4	Enter wordcount for the name of the package and click the Finish button.
2	Create the Mapper Class
2.1	Right-click on the wordcount package and select New -> Class.
2.2	Name the class WordCountMapper. Have it extend the org.apache.hadoop.mapreduce.Mapper class, as shown here:

2.3	Click the Finish button and the new class will open in the Eclipse editor.
2.4	Notice your new class has errors because you need to specify the generics. Your mapper is going to receive input from the TextInputFormat class.What is the data type of the key generated by TextInputFormat? ____________ Answer: LongWritableWhat is the data type of the value? _______________Answer: Text
2.5	Change KEYIN to LongWritable and VALUEIN to Text.
2.6	The WordCountMapper is going to output a string followed by the number of occurrences of that string as an integer. Therefore, change the KEYOUT generic to Text and the VALUEOUT generic to IntWritable . Your class declaration should now look like:
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>
3	Define the map Method
3.1	Right-click in the Eclipse editor of WordCountMapper.java and select Source -> Override/Implement Methodsâ€¦.
3.2	Select the map method from the list of methods, and then click the OK button. The map method will appear in the class. Delete the code and comments from map that were automatically generated.
3.3	The Text parameter named value contains a line of text that was read from the file.What does the value of key represent?___________________________________Answer: The offset into the file
3.4	Inside the map method, use the toString method of value to retrieve the String object within the Text parameter:
String currentLine = value.toString();
3.5	Parse the line of text into individual words using the split method of the StringUtils class. (The words are separated by spaces):
String [] words = StringUtils.split(currentLine, ' ');
3.6	Write a for loop that iterates through the words array. Within the for loop, use the context.write method to output a key/value pair that contains the word as the key (in a Text object) and the number 1 (as an IntWritable):
for(String word : words) {
Text outputKey = new Text(word);
IntWritable one = new IntWritable(1);
context.write(outputKey, one);
}
3.7	Now review the map method - it looks like a typical Java method. You instantiate a couple of objects and they get serialized and passed on. However, recall how often this map method will get invoked: once for each line of text in your big data. This could be millions or billions of method calls.In the world of MapReduce programming, you need to take advantage of any optimization possible. Instantiating a Text and IntWritable object that will just be immediately garbage collected is going to be expensive. Instead, you should reuse a single instance of these objects for each call to context.write. A good design is to instantiate these objects outside of the map method, which you will do now. Add the following two fields to your WordCountMapper class:
private static final IntWritable ONE =
                                new IntWritable(1);
private Text outputKey = new Text();
3.8	Modify the for loop in your map method so that it uses the existing Text object and IntWritable object:
for(String word: words) {
outputKey.set(word);
context.write(outputKey, ONE);
}
3.9	Save your changes to WordCountMapper.java.
4	Create the Reducer Class
4.1	Add a class named WordCountReducer to the wordcount package. Have the class extend org.apache.hadoop.mapreduce.Reducer.
4.2	Based on WordCountMapper, what does the data type of KEYIN have to be? ___________________ Answer: TextAnd VALUEIN? ____________________Answer: IntWritable
4.3	The WordCountReducer is going to output the Text that is passed in, along with the sum (as an IntWritable) of how many occurrences that the Text appeared, so change the declaration of the class to be:
public class WordCountReducer
  extends Reducer<Text, IntWritable, Text, IntWritable>
4.4	The output value is an IntWritable instance. We can reuse one instance, instead of instantiating a new one each time reduce is invoked. Add the following field to WordCountReducer:
private IntWritable outputValue = new IntWritable();
4.5	Add the reduce method declaration:
@Override
protected void reduce(Text key,
                      Iterable<IntWritable> values,
         Context context)
throws IOException, InterruptedException {
}
4.6	Within the reduce method, add a for loop that iterates through the IntWritable collection of values and sums them together into a single int:
int sum = 0;
for(IntWritable count : values) {
sum += count.get();
}
4.7	Output the key/value pair of the word along with its total number of occurrences:
outputValue.set(sum);
context.write(key, outputValue);
4.8	Save your changes to WordCountReducer.java.
5	Write the Configured/Tool Class
5.1	Add a new class named WordCountJob to the wordcount package. Have the class extend org.apache.hadoop.conf.Configured and implement org.apache.hadoop.util.Tool, and also include the main method:

6	Implement the run Method
6.1	Within the run method of WordCountJob, create a new org.apache.hadoop.mapreduce.Job object and set its JAR class to be WordCount.class (using the getClass ( ) method). The Configuration class is in the org.apache.hadoop.conf package.
Job job = Job.getInstance(getConf(), "WordCountJob");
Configuration conf = job.getConfiguration();
job.setJarByClass(getClass());
6.2	Set the input and output paths to be the first two command-line arguments:
Path in = new Path(args[0]);
Path out = new Path(args[1]);
FileInputFormat.setInputPaths(job, in);
FileOutputFormat.setOutputPath(job, out);
6.3	Set the mapper to be WordCountMapper and the reducer to be WordCountReducer:
job.setMapperClass(WordCountMapper.class);
job.setReducerClass(WordCountReducer.class);
6.4	Use TextInputFormat and TextOutputFormat (from the org.apache.hadoop.mapreduce.lib.input and org.apache.hadoop.mapreduce.lib.output packages) for the format of the input and output data:
job.setInputFormatClass(TextInputFormat.class);
job.setOutputFormatClass(TextOutputFormat.class);
6.5	Configure the data types of the output of the Mapper:
job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(IntWritable.class);
6.6	Configure the data types of the output of the Reducer:
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
6.7	Submit the MapReduce job:
return job.waitForCompletion(true)?0:1;
6.8	Save your changes to WordCount.java and make sure you do not have any compiler errors.
7	Implement the main Method
7.1	Within the main method of WordCountJob, use ToolRunner to run your Tool (i.e. WordCountJob) instance:
int result = ToolRunner.run(new Configuration(),  
                            new WordCountJob(), args);
System.exit(result);
7.2	Surround the code above with a try/catch block (or have main declare Exception).
7.3	Save your changes to WordCountJob.java.
8	Build the JAR
8.1	Right-click on the WordCount project and select Run As -> Gradle Build.
8.2	Make sure the WordCount project builds successfully.
8.3	Open a Terminal window and change directories to ~/java/labs/Lab2.1:
# cd ~/java/labs/Lab2.1
8.4	Put the file constitution.txt into HDFS:
# hadoop fs -put constitution.txt
8.5	Change directories to the WordCount project folder:
# cd ~/java/workspace/WordCount
8.6	Run the program by entering the following command all on a single line:
# yarn jar wordcount.jar constitution.txt wordcount_output
Wait for the MapReduce to finish.
9	View the Results
9.1	Verify you have a folder named wordcount_output:
# hadoop fs -ls wordcount_output
9.2	View the contents of the file named wordcount_output /part-r-00000. You should see the word counts of the U.S. Constitution. Notice the words appear in alphabetical order. Why? ____________________________________________________________ Answer: The key for this MapReduce job was the word, and records get sorted by the key during the shuffle/sort phase.
10	Run the Program Again
10.1	Run the same yarn jar command to execute the WordCountJob application again.
10.2	This time you will get a FileAlreadyExistsException, which is thrown when Hadoop tries to create the folder named wordcount_output . This is a common issue during development.
10.3	Delete the wordcount_output folder in HDFS:
# hadoop fs -rm -R wordcount_output
10.4	Run WordCountJob again. This time the job should run successfully.
10.5	To delete a folder from your application, add the following code to the run method of WordCountJob (after you instantiate the out object):
out.getFileSystem(conf).delete(out, true);
10.6	Save your changes to WordCountJob.java and run the job again. The job should run successfully this time, even though the wordcount_output folder already existed.
 	Result: Congratulations, you have just written your first MapReduce application. You could use this same code in a production environment, if you needed to count the number of occurrences of words in your Big Data files.
Demo Steps
1	Start the Grunt Shell
1.1	Review the contents of the file pigdemo.txt , located in /root/java/labs/demos.
1.2	Start the Grunt shell:
root@ubuntu:~/java/labs/demos# pig
1.3	Notice the output includes where the logging for your Pig session will go as well as a statement about connecting to your Hadoop file system:
[main] INFO  org.apache.pig.Main - Logging error messages to: /root/java/labs/demos/pig_1377892197767.log
[main] INFO  org.apache.pig.backend.hadoop.executionengine. HExecutionEngine - Connecting to hadoop file system at: hdfs://namenode:8020
2	Make a New Directory
2.1	Notice you can run HDFS commands easily from the Grunt shell. For example, run the ls command:
grunt> ls
2.2	Make a new directory named demos:
grunt> mkdir demos
2.3	Use copyFromLocal to copy the pigdemo.txt file into the demos folder:
grunt> copyFromLocal /root/java/labs/demos/pigdemo.txt demos/
2.4	Verify that the file was uploaded successfully:
grunt> ls demos
hdfs://namenode:8020/user/root/demos/pigdemo.txt<r 3>89
2.5	Change the present working directory to demos:
grunt> cd demos
grunt> pwd
hdfs://namenode:8020/user/root/demos
2.6	View the contents using the cat command:
grunt> cat pigdemo.txt
SD      Rich
NV      Barry
CO      George
CA      Ulf
IL      Danielle
OH      Tom
CA      manish
CA      Brian
CO      Mark
3	Define a Relation
3.1	Define the employees relation, using a schema:
grunt> employees = LOAD 'pigdemo.txt' AS (state, name);
3.2	Demonstrate the describe command, which describes what a relation looks like:
grunt> describe employees;
employees: {state: bytearray,name: bytearray}
3.3	Let’s view the records in the employees relation:
grunt> DUMP employees;
Notice this requires a MapReduce job to execute, and the result is a collection of tuples:
(SD,Rich)
(NV,Barry)
(CO,George)
(CA,Ulf)
(IL,Danielle)
(OH,Tom)
(CA,manish)
(CA,Brian)
(CO,Mark)
4	Filter the Relation by a Field
4.1	Let’s filter the employees whose state field equals CA:
grunt> ca_only = FILTER employees BY (state=='CA');
grunt> DUMP ca_only;
4.2	The output is still tuples, but only the records that match the filter appear:
(CA,Ulf)
(CA,manish)
(CA,Brian)
5	Create a Group
5.1	Define a relation that groups the employees by the state field:
grunt> emp_group = GROUP employees BY state;
5.2	Bags represent groups in Pig. A bag is an unordered collection of tuples:
grunt> describe emp_group;
emp_group: {group: bytearray,employees: {(state: bytearray,name: bytearray)}}
5.3	All records with the same state will be grouped together, as shown by the output of the emp_group relation:
grunt> DUMP emp_group;
The output is:
(CA,{(CA,Ulf),(CA,manish),(CA,Brian)})
(CO,{(CO,George),(CO,Mark)})
(IL,{(IL,Danielle)})
(NV,{(NV,Barry)})
(OH,{(OH,Tom)})
(SD,{(SD,Rich)})
6	The STORE Command
6.1	The DUMP command dumps the contents of a relation to the console. The STORE command sends the output to a folder in HDFS. For example:
grunt> STORE emp_group INTO 'emp_group';
Notice at the end of the MapReduce job that no records are output to the console.
6.2	Verify that a new folder is created:
grunt> ls
hdfs://namenode:8020/user/root/demos/emp_group<dir>
hdfs://namenode:8020/user/root/demos/pigdemo.txt<r 3>89
6.3	View the contents of the output file:
grunt> cat emp_group/part-r-00000
CA      {(CA,Ulf),(CA,manish),(CA,Brian)}
CO      {(CO,George),(CO,Mark)}
IL      {(IL,Danielle)}
NV      {(NV,Barry)}
OH      {(OH,Tom)}
SD      {(SD,Rich)}
Notice that a tab character separates the fields of the records (which in this case is the state field followed by a bag), which is the default delimiter in Pig. Use the PigStorage object to specify a different delimiter:
grunt> STORE emp_group INTO 'emp_group_csv' USING PigStorage(',');
7	Show All Aliases
7.1	The aliases command shows a list of currently defined aliases:
grunt> aliases;
aliases: [ca_only, emp_group, employees]
8	Monitor the Pig Jobs
8.1	Point your browser to the JobHistory UI at http://resourcemanager :19888 /.
8.2	View the list of jobs, which should contain the MapReduce jobs that were executed from your Pig Latin code in the Grunt shell.
8.3	Notice you can view the log files of the ApplicationMaster and of each Map and Reduce task.